# kaggledays--meetup-berlin

We trained tf_efficientnetb3_ns on the images and used the output of GAP layer to train light GBM to get the final predictions.
We were positioned 6th in the competition with private LB score: 0.18541

[Train code for Efficientnetb3](b3_training.ipynb)

[Train code for LGBM](lgb_training_b3)

[Inference code](lgb-inf-b3)


</br>


[Weights for Efficientnetb3](https://www.kaggle.com/datasets/darkravager/tf-efficientnet-b3-ns)

[Weights for LGBM](https://www.kaggle.com/datasets/sakshamaggarwal/lgbm-kaggle)




